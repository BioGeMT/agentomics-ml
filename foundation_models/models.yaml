ESM-2:
  models:
    - name: facebook/esm2_t6_8M_UR50D
      params: 8M
    - name: facebook/esm2_t12_35M_UR50D
      params: 35M
    - name: facebook/esm2_t30_150M_UR50D
      params: 150M
    - name: facebook/esm2_t33_650M_UR50D
    - name: facebook/esm2_t36_3B_UR50D
      params: 3B  
    - name: facebook/esm2_t48_15B_UR50D
      params: 15B
  summary: ESM-2 is a protein language model trained on the UniRef protein sequence database using a masked language modeling objective to learn evolutionary and structural dependencies between amino acids.
  path_to_info: /foundation_models/ESM-2.md

HyenaDNA:
  models:
    - name: LongSafari/hyenadna-tiny-1k-seqlen-hf
      params: 1K
    - name: LongSafari/hyenadna-small-32k-seqlen-hf
      params: 32K
    - name: LongSafari/hyenadna-medium-160k-seqlen-hf
      params: 160K
    - name: LongSafari/hyenadna-medium-450k-seqlen-hf
      params: 450K
    - name: LongSafari/hyenadna-large-1m-seqlen-hf
      params: 1M
  summary: HyenaDNA is a genomic language model trained on the human reference genome using next-nucleotide prediction, enabling long-context modeling and single-nucleotide resolution for diverse genomic prediction tasks.
  path_to_info: /foundation_models/HyenaDNA.md

NucleotideTransformer:
  models:
    - name: InstaDeepAI/nucleotide-transformer-v2-50m-multi-species
      params: 50M
    - name: InstaDeepAI/nucleotide-transformer-v2-100m-multi-species
      params: 100M
    - name: InstaDeepAI/nucleotide-transformer-v2-250m-multi-species
      params: 250M
    - name: InstaDeepAI/nucleotide-transformer-v2-500m-multi-species
      params: 500M
    - name: InstaDeepAI/nucleotide-transformer-500m-1000g
      params: 500M
    - name: InstaDeepAI/nucleotide-transformer-2.5b-multi-species
      params: 2.5B
    - name: InstaDeepAI/nucleotide-transformer-2.5b-1000g
      params: 2.5B
  summary: NucleotideTransformer is a genomic language model trained on DNA sequences from 850 diverse species using a masked language modeling objective, enabling cross-species genomic representation learning for diverse genomic prediction tasks.
  path_to_info: /foundation_models/NucleotideTransformer.md

rinalmo:
  models:
    - name: multimolecule/rinalmo-micro
      params: 33M
    - name: multimolecule/rinalmo-mega
      params: 148M
    - name: multimolecule/rinalmo-giga
      params: 650M
  summary: RiNALMo is an RNA language model trained on 36 million non-coding RNA sequences using a masked language modeling objective, enabling the extraction of structural and functional information from raw nucleotide data.
  path_to_info: /foundation_models/rinalmo.md